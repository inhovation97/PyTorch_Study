{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> Lab 02: Linear Regression 실습 <center/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ H(x) = Wx + b $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ cost(W, b) = \\frac{1}{m} \\sum^m_{i=1} \\left( H(x^{(i)}) - y^{(i)} \\right)^2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - $H(x)$: 주어진 $x$ 값에 대해 예측을 어떻게 할 것인가\n",
    " - $cost(W, b)$: $H(x)$ 가 $y$ 를 얼마나 잘 예측했는가"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모듈을 사용하지 않은 단일 선형 회귀 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/1000 W: 0.093, b: 0.040 Cost: 4.666667\n",
      "Epoch  100/1000 W: 0.873, b: 0.289 Cost: 0.012043\n",
      "Epoch  200/1000 W: 0.900, b: 0.227 Cost: 0.007442\n",
      "Epoch  300/1000 W: 0.921, b: 0.179 Cost: 0.004598\n",
      "Epoch  400/1000 W: 0.938, b: 0.140 Cost: 0.002842\n",
      "Epoch  500/1000 W: 0.951, b: 0.110 Cost: 0.001756\n",
      "Epoch  600/1000 W: 0.962, b: 0.087 Cost: 0.001085\n",
      "Epoch  700/1000 W: 0.970, b: 0.068 Cost: 0.000670\n",
      "Epoch  800/1000 W: 0.976, b: 0.054 Cost: 0.000414\n",
      "Epoch  900/1000 W: 0.981, b: 0.042 Cost: 0.000256\n",
      "Epoch 1000/1000 W: 0.985, b: 0.033 Cost: 0.000158\n"
     ]
    }
   ],
   "source": [
    "# import\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "# pytorch의 random seed를 고정할 때 쓰는 함수\n",
    "torch.manual_seed(1)\n",
    "\n",
    "# 데이터\n",
    "x_train = torch.FloatTensor([[1], [2], [3]])\n",
    "y_train = torch.FloatTensor([[1], [2], [3]])\n",
    "# 모델 초기화\n",
    "W = torch.zeros(1, requires_grad=True)  # requires_grad=True는 epochs마다 메모리에 연산했던 것들을 track하는 것.\n",
    "b = torch.zeros(1, requires_grad=True)\n",
    "# optimizer 설정\n",
    "optimizer = optim.SGD([W, b], lr=0.01)  # SGD로 optimizer 정의, learning rate는 0.01 \n",
    "\n",
    "nb_epochs = 1000\n",
    "for epoch in range(nb_epochs + 1):\n",
    "    \n",
    "    # H(x) 계산\n",
    "    hypothesis = x_train * W + b\n",
    "    \n",
    "    # cost 계산\n",
    "    cost = torch.mean((hypothesis - y_train) ** 2)\n",
    "\n",
    "    # cost로 H(x) 개선\n",
    "    # 항상 붙어다니는 3줄\n",
    "    optimizer.zero_grad()  # gradient 초기화\n",
    "    cost.backward()  # 기울기 계산\n",
    "    optimizer.step() # 경사하강법으로 최적화\n",
    "\n",
    "    # 100번마다 로그 출력\n",
    "    if epoch % 100 == 0:\n",
    "        print('Epoch {:4d}/{} W: {:.3f}, b: {:.3f} Cost: {:.6f}'.format(\n",
    "            epoch, nb_epochs, W.item(), b.item(), cost.item()\n",
    "        ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 초기화한 w와 b, 목적 함수, 손실 함수 전부 정의해야됨.   \n",
    "+ torch.optim()을 써서 최적화를 진행함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `nn.Module`을 사용한 단일 선형 회귀 코드"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 먼저 모델 클래스 만들어주기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 클래스 만들기\n",
    "class LinearRegressionModel(nn.Module):  # nn.model 클래스를 상속받음\n",
    "    def __init__(self):\n",
    "        super().__init__()                # 상속받은 nn.model의 함수를 이용\n",
    "        self.linear = nn.Linear(1, 1)     # 단순 선형 회귀이므로 input=1, ouput=1\n",
    "\n",
    "    def forward(self, x):                 # 모델이 학습 데이터를 받아서 forward시키는 함수\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.1471, grad_fn=<MseLossBackward>)\n",
      "Epoch    0/1000 W: 0.578, b: -0.413 Cost: 2.147149\n",
      "Epoch  100/1000 W: 1.066, b: -0.150 Cost: 0.003239\n",
      "Epoch  200/1000 W: 1.052, b: -0.118 Cost: 0.002002\n",
      "Epoch  300/1000 W: 1.041, b: -0.093 Cost: 0.001237\n",
      "Epoch  400/1000 W: 1.032, b: -0.073 Cost: 0.000764\n",
      "Epoch  500/1000 W: 1.025, b: -0.057 Cost: 0.000472\n",
      "Epoch  600/1000 W: 1.020, b: -0.045 Cost: 0.000292\n",
      "Epoch  700/1000 W: 1.016, b: -0.035 Cost: 0.000180\n",
      "Epoch  800/1000 W: 1.012, b: -0.028 Cost: 0.000111\n",
      "Epoch  900/1000 W: 1.010, b: -0.022 Cost: 0.000069\n",
      "Epoch 1000/1000 W: 1.008, b: -0.017 Cost: 0.000043\n"
     ]
    }
   ],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "# import torch.optim as optim\n",
    "\n",
    "# 데이터\n",
    "x_train = torch.FloatTensor([[1], [2], [3]])\n",
    "y_train = torch.FloatTensor([[1], [2], [3]])\n",
    "\n",
    "    \n",
    "# 모델 초기화\n",
    "model = LinearRegressionModel()\n",
    "\n",
    "    \n",
    "# hypothesis, cost 함수 정의\n",
    "hypothesis = model(x_train)\n",
    "cost = F.mse_loss(hypothesis, y_train) # 이번에는 간단하게 내장 함수를 이용함.(직접 수식 구현 x)\n",
    "print(cost)\n",
    "\n",
    "\n",
    "# optimizer\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01) # 최적화 기법으로 SGD 채택    \n",
    "    \n",
    "    \n",
    "    \n",
    "# optimizer 설정\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "nb_epochs = 1000\n",
    "for epoch in range(nb_epochs + 1):\n",
    "    \n",
    "    # H(x) 계산\n",
    "    prediction = model(x_train)\n",
    "    \n",
    "    # cost 계산\n",
    "    cost = F.mse_loss(prediction, y_train)\n",
    "    # cost로 H(x) 개선\n",
    "    optimizer.zero_grad() # gradient 초기화\n",
    "    cost.backward()       # 기울기 계산\n",
    "    optimizer.step()      # 경사하강법으로 최적화 \n",
    "    \n",
    "    # 100번마다 로그 출력\n",
    "    if epoch % 100 == 0:\n",
    "        params = list(model.parameters())\n",
    "        W = params[0].item()\n",
    "        b = params[1].item()\n",
    "        print('Epoch {:4d}/{} W: {:.3f}, b: {:.3f} Cost: {:.6f}'.format(\n",
    "            epoch, nb_epochs, W, b, cost.item()\n",
    "        ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### cost가 점점 줄어듦"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
